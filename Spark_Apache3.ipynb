{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJrmw21+/yedbrzw8MKa3h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoya-ivanova/BigData/blob/main/Spark_Apache3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Apache\n",
        "\n",
        "### Инструменты работы и визуализации ч.2\n",
        "\n",
        "Условие: есть набор данных о продажах продуктов с информацией о дате продаж, категории продукта, количестве и выручке от продаж.\n",
        "\n",
        "Используя Apache Spark, загрузите предоставленный набор данных в DataFrame (пример данных ниже).\n",
        "\n",
        "(\"2023-11-20\", \"Electronics\", 100, 12000),\n",
        "(\"2023-11-21\", \"Electronics\", 110, 13000),\n",
        "(\"2023-11-22\", \"Electronics\", 105, 12500),\n",
        "(\"2023-11-20\", \"Clothing\", 300, 15000),\n",
        "(\"2023-11-21\", \"Clothing\", 280, 14000),\n",
        "(\"2023-11-22\", \"Clothing\", 320, 16000),\n",
        "(\"2023-11-20\", \"Books\", 150, 9000),\n",
        "(\"2023-11-21\", \"Books\", 200, 12000),\n",
        "(\"2023-11-22\", \"Books\", 180, 10000)\n",
        "\n",
        "Столбцы: \"date\", \"category\", \"quantity\", \"revenue\".\n",
        "\n",
        "С использованием оконных функций, рассчитайте среднее выручки от продаж для каждой категории продукта.\n",
        "Примените операцию pivot для того, чтобы преобразовать полученные данные таким образом, чтобы в качестве строк были категории продуктов, в качестве столбцов были дни, а значениями были средние значения выручки от продаж за соответствующий день"
      ],
      "metadata": {
        "id": "rqmIGkPPhYgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aQja2li1hEg9",
        "outputId": "4bc3a3b0-0370-4ca0-8de1-5dd361092939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=22f47ae08bd34c28ee897c8b2ceea26d0e7dcda6b96d8b31567b6f6b88bc8603\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg"
      ],
      "metadata": {
        "id": "T_rk4nX_nRvK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"WindowFunctionExample\").getOrCreate()\n",
        "\n",
        "# Создаем DF\n",
        "data = [(\"2023-11-20\", \"Electronics\", 100, 12000), (\"2023-11-21\", \"Electronics\", 110, 13000), (\"2023-11-22\", \"Electronics\", 105, 12500),\n",
        "  (\"2023-11-20\", \"Clothing\", 300, 15000), (\"2023-11-21\", \"Clothing\", 280, 14000), (\"2023-11-22\", \"Clothing\", 320, 16000),\n",
        "  (\"2023-11-20\", \"Books\", 150, 9000), (\"2023-11-21\", \"Books\", 200, 12000), (\"2023-11-22\", \"Books\", 180, 10000)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"date\", \"category\", \"quantity\", \"revenue\"])\n",
        "\n",
        "df.show()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm8dk1uvbWW-",
        "outputId": "e353669e-1463-4208-ce29-a7578236f5ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+--------+-------+\n",
            "|      date|   category|quantity|revenue|\n",
            "+----------+-----------+--------+-------+\n",
            "|2023-11-20|Electronics|     100|  12000|\n",
            "|2023-11-21|Electronics|     110|  13000|\n",
            "|2023-11-22|Electronics|     105|  12500|\n",
            "|2023-11-20|   Clothing|     300|  15000|\n",
            "|2023-11-21|   Clothing|     280|  14000|\n",
            "|2023-11-22|   Clothing|     320|  16000|\n",
            "|2023-11-20|      Books|     150|   9000|\n",
            "|2023-11-21|      Books|     200|  12000|\n",
            "|2023-11-22|      Books|     180|  10000|\n",
            "+----------+-----------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисляем среднюю выручку для каждой категории продуктов по дням\n",
        "df_avg_revenue = df.groupBy(\"category\", \"date\").agg(avg(\"revenue\").alias(\"avg_revenue\"))\n",
        "\n",
        "# Выполняем операцию pivot, чтобы получить таблицу с категориями в строках и днями в столбцах\n",
        "df_pivot = df_avg_revenue.groupBy(\"category\").pivot(\"date\").agg(avg(\"avg_revenue\"))\n",
        "\n",
        "# Выводим результирующий DataFrame\n",
        "df_pivot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3T3YgfscLan",
        "outputId": "f5f25bba-0234-4639-8d3a-82c28883694f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+----------+----------+\n",
            "|   category|2023-11-20|2023-11-21|2023-11-22|\n",
            "+-----------+----------+----------+----------+\n",
            "|Electronics|   12000.0|   13000.0|   12500.0|\n",
            "|   Clothing|   15000.0|   14000.0|   16000.0|\n",
            "|      Books|    9000.0|   12000.0|   10000.0|\n",
            "+-----------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Останавливаем SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "qoI5b8wyhNbg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Apache (семинары)\n",
        "Инструменты работы и визуализации ч.2"
      ],
      "metadata": {
        "id": "NcbytI4LcMCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"WindowFunctionExample\").getOrCreate()\n",
        "\n",
        "# Создаем DF\n",
        "data = [(\"A\", 1), (\"A\", 2), (\"A\", 3), (\"B\", 4), (\"B\", 5), (\"B\", 6)]\n",
        "df = spark.createDataFrame(data, [\"Group\", \"Value\"])\n",
        "\n",
        "# Определяем окно для каждой группы\n",
        "windowSpec = Window.partitionBy(\"Group\").orderBy(\"Value\")\n",
        "\n",
        "# Вычисление скользящего среднего\n",
        "df = df.withColumn(\"MovingAverage\", avg(\"Value\").over(windowSpec.rowsBetween(-2, 0)))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Js53v1oEZT",
        "outputId": "9d22f59c-5418-431d-d3e6-59b05f85963d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------------+\n",
            "|Group|Value|MovingAverage|\n",
            "+-----+-----+-------------+\n",
            "|    A|    1|          1.0|\n",
            "|    A|    2|          1.5|\n",
            "|    A|    3|          2.0|\n",
            "|    B|    4|          4.0|\n",
            "|    B|    5|          4.5|\n",
            "|    B|    6|          5.0|\n",
            "+-----+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Типы оконных функций:\n",
        "\n",
        "1/ Ранжирующие\n",
        "row_number\n",
        "rank\n",
        "percent_rank\n",
        "dense_rank\n",
        "ntile\n",
        "\n",
        "2/ Аналитические\n",
        "lag\n",
        "lead\n",
        "cume_dist\n",
        "nth_value\n",
        "\n",
        "3/ Аггрегирующие\n",
        "min/max\n",
        "count\n",
        "std\n",
        "\n",
        "### Параметры:\n",
        "\n",
        "1.PartitionBy()\n",
        "\n",
        "2.OrderBy()\n",
        "\n",
        "3.rowsBetweens(start, end)\n",
        "\n",
        "4.rangeBetwen(start, end)\n",
        "\n",
        "Window.unbdoudedPreceding, Window.unboudedFolliwing, Window.currentRow\n",
        "\n",
        "\n",
        "### Окна нужны:\n",
        "\n",
        "1 Гибкость в анализе данных\n",
        "\n",
        "2 Сохранение идентичности исх данных\n",
        "\n",
        "3 Поддержка сложных аналит запросов\n",
        "\n",
        "4 Оптимизация производительности\n",
        "\n",
        "5 Поддержка различных типов окон\n"
      ],
      "metadata": {
        "id": "PGE3jvYrrjxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Определение окна с разделением на партиции по столбцу \"country\" и ортировкой по столбцу \"data\"\n",
        "window = Window.partitionBy(\"country\").orderBy(\"data\")\n",
        "\n",
        "# Определение окна с границами, охватывающими строки от начала до текущей строки\n",
        "window = Window.orderBy(\"data\").rowsBetween(Window.unbdoudedPreceding, Window.currentRow)\n",
        "\n",
        "# Определение окна с границами, охватывающими строки от 3 до текущей строки\n",
        "window = Window.orderBy(\"data\").rowsBetween(-3, Window.currentRow)\n",
        "\n",
        "# Определение окна с границами, охватывающими строки от 3 строк до 3 строк после текущей строки\n",
        "window = Window.orderBy(\"data\").rowsBetween(-3, -3)"
      ],
      "metadata": {
        "id": "r89L5kpcv7xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Определение окна с разделением на партиции по столбцу \"departament\" и ортировкой по столбцу \"salary\"\n",
        "windowSpec = Window.partitionBy(\"departament\").orderBy(\"salary\")\n",
        "\n",
        "# Применение оконной функции avg с использованием rowsBetween\n",
        "df.withColumn(\"avg_salary\", avg(\"salary\").over(windowSpec.rowsBetween(Window.unbdoudedPreceding, Window.currentRow)))\n",
        "\n",
        "# Применение оконной функции avg с использованием rangeBetween\n",
        "df.withColumn(\"avg_salary_range\", avg(\"salary\").over(windowSpec.rowsBetween(Window.unbdoudedPreceding, 1000)))"
      ],
      "metadata": {
        "id": "UFosGxFI5vkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rowsBetween(Window.correntRow, 1)  # есть некое окно, включающее одну строки и одну после нее\n",
        "rangeBetween(Window.correntRow, 1)  # есть некое окно, включающее строки значения при этом в столбце которого в пределах 1 от тек значения"
      ],
      "metadata": {
        "id": "C8SSwE178CIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as func\n",
        "\n",
        "# Определение окна с использованием rowsBetween\n",
        "window1 = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.correntRow, 1)\n",
        "\n",
        "# Определение окна с использованием rangeBetween\n",
        "window2 = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.correntRow, 1)\n",
        "\n",
        "# Применение оконной функции DF\n",
        "df.withColumn(\"sum_rows\", func.sum(\"id\").over(window1)) \\\n",
        "  .withColumn(\"sum_range\", func.sum(\"id\").over(window2)) \\\n",
        "  .show()"
      ],
      "metadata": {
        "id": "n2NPZkuR9X7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Определение пользовательской функции\n",
        "def add_one(x):\n",
        "  return x + 1\n",
        "\n",
        "# Преобразование функции в UDF\n",
        "add_one_udf = udf(add_one, IntegerType())\n",
        "\n",
        "# Использование UDF в DF\n",
        "df = spark.createDataFrame([(1,), (2,), (3,)], ['value'])\n",
        "df.withColumn(\"value_plus_one\", add_one_udf(df[\"value\"])).show()"
      ],
      "metadata": {
        "id": "k3FNVcQVB0Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "входные/выходные данные для UDF\n",
        "\n",
        "1/IntergerType\n",
        "\n",
        "2/DoubleType\n",
        "\n",
        "3/StringType\n",
        "\n",
        "4/BooleanType\n",
        "\n",
        "5/ArrayType\n",
        "\n",
        "6/MapType\n",
        "\n",
        "7/StructType"
      ],
      "metadata": {
        "id": "Jr9nbcYRELXx"
      }
    }
  ]
}