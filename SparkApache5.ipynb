{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VtggnLn_-HUq",
        "alAKvc-c91l_",
        "i1Cm5nVo-z6h",
        "Dc--9guc-rL3",
        "pgA14vPX_FSs",
        "5VwGK-B6_KrX",
        "VgbSwOTcABqu",
        "YoQRHVhPAMmW",
        "kYWcfsR5Ahmy",
        "erFriTM-E-56"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoya-ivanova/BigData/blob/main/SparkApache5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Apache\n",
        "\n",
        "### Cоздай csv файл с содержимым:\n",
        "\n",
        "title,author,genre,sales,year\n",
        "\n",
        "\"1984\", \"George Orwell\", \"Science Fiction\", 5000, 1949\n",
        "\n",
        "\"The Lord of the Rings\", \"J.R.R. Tolkien\", \"Fantasy\", 3000, 1954\n",
        "\n",
        "\"To Kill a Mockingbird\", \"Harper Lee\", \"Southern Gothic\", 4000, 1960\n",
        "\n",
        "\"The Catcher in the Rye\", \"J.D. Salinger\", \"Novel\", 2000, 1951\n",
        "\n",
        "\"The Great Gatsby\", \"F. Scott Fitzgerald\", \"Novel\", 4500, 1925\n",
        "\n",
        "### Используя Spark:\n",
        "\n",
        "— Прочитайте данные из файла csv.\n",
        "\n",
        "— Фильтруйте данные, чтобы оставить только книги, продажи которых превышают 3000 экземпляров.\n",
        "\n",
        "— Сгруппируйте данные по жанру и вычислите общий объем продаж для каждого жанра.\n",
        "\n",
        "— Отсортируйте данные по общему объему продаж в порядке убывания.\n",
        "\n",
        "— Выведите результаты на экран."
      ],
      "metadata": {
        "id": "8FTQGD5JcTC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Данные для CSV файла передаем в переменную books\n",
        "books = [\n",
        "    [\"1984\", \"George Orwell\", \"Science Fiction\", 5000, 1949],\n",
        "    [\"The Lord of the Rings\", \"J.R.R. Tolkien\", \"Fantasy\", 3000, 1954],\n",
        "    [\"To Kill a Mockingbird\", \"Harper Lee\", \"Southern Gothic\", 4000, 1960],\n",
        "    [\"The Catcher in the Rye\", \"J.D. Salinger\", \"Novel\", 2000, 1951],\n",
        "    [\"The Great Gatsby\", \"F. Scott Fitzgerald\", \"Novel\", 4500, 1925]\n",
        "]"
      ],
      "metadata": {
        "id": "7kOzD2GQc45I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание CSV файла\n",
        "with open('books.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"title\", \"author\", \"genre\", \"sales\", \"year\"])  # Заголовок\n",
        "    writer.writerows(books)"
      ],
      "metadata": {
        "id": "w_A8OgwVetxs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем SparkSession\n",
        "spark = SparkSession.builder.appName(\"BookSalesAnalysis\").getOrCreate()"
      ],
      "metadata": {
        "id": "7yGXrj42e1Gj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Считываем данные из CSV файла\n",
        "df = spark.read.csv(\"books.csv\", header=True, inferSchema=True)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFQ22FqIetnw",
        "outputId": "e38f7904-4519-4f32-894d-eb1190b58373"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+---------------+-----+----+\n",
            "|               title|             author|          genre|sales|year|\n",
            "+--------------------+-------------------+---------------+-----+----+\n",
            "|                1984|      George Orwell|Science Fiction| 5000|1949|\n",
            "|The Lord of the R...|     J.R.R. Tolkien|        Fantasy| 3000|1954|\n",
            "|To Kill a Mocking...|         Harper Lee|Southern Gothic| 4000|1960|\n",
            "|The Catcher in th...|      J.D. Salinger|          Novel| 2000|1951|\n",
            "|    The Great Gatsby|F. Scott Fitzgerald|          Novel| 4500|1925|\n",
            "+--------------------+-------------------+---------------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Фильтруем данные, чтобы оставить только книги, продажи которых превышают 3000 экземпляров\n",
        "filtered_df = df.filter(col(\"sales\") > 3000)\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP0fdKtoetkk",
        "outputId": "7d263aaa-3984-4e17-992a-58b37c432733"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+---------------+-----+----+\n",
            "|               title|             author|          genre|sales|year|\n",
            "+--------------------+-------------------+---------------+-----+----+\n",
            "|                1984|      George Orwell|Science Fiction| 5000|1949|\n",
            "|To Kill a Mocking...|         Harper Lee|Southern Gothic| 4000|1960|\n",
            "|    The Great Gatsby|F. Scott Fitzgerald|          Novel| 4500|1925|\n",
            "+--------------------+-------------------+---------------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сгруппируем данные по жанру и вычислим общий объем продаж для каждого жанра\n",
        "grouped_df = filtered_df.groupBy(\"genre\").agg(sum(\"sales\").alias(\"total_sales\"))\n",
        "grouped_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPWDWwj0etdY",
        "outputId": "60b607a2-4e1f-4d85-db2f-ab4d3e4f8c29"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+\n",
            "|          genre|total_sales|\n",
            "+---------------+-----------+\n",
            "|Southern Gothic|       4000|\n",
            "|          Novel|       4500|\n",
            "|Science Fiction|       5000|\n",
            "+---------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отсортируем данные по общему объему продаж в порядке убывания\n",
        "sorted_df = grouped_df.orderBy(col(\"total_sales\").desc())\n",
        "sorted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sR0nHZxetPQ",
        "outputId": "02124435-0431-40ed-94f7-07da2bb97020"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+\n",
            "|          genre|total_sales|\n",
            "+---------------+-----------+\n",
            "|Science Fiction|       5000|\n",
            "|          Novel|       4500|\n",
            "|Southern Gothic|       4000|\n",
            "+---------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Остановка SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "9eLBbynBgzUD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark on scala -->\n"
      ],
      "metadata": {
        "id": "7i6irpFPkhA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark >> None"
      ],
      "metadata": {
        "id": "-yDXhxrk9Zgs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y scala"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0fQymk7SjlbK",
        "outputId": "9e6cbb07-397e-4eb8-91a6-f91fdb92e847"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "scala is already the newest version (2.11.12-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spylon-kernel  # код для установки ядра Scala"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LNpfhCTEmGj-",
        "outputId": "89cece72-8c18-41a1-ac9a-faf81d9cf66f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spylon-kernel in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from spylon-kernel) (5.5.6)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.10/dist-packages (from spylon-kernel) (0.19.1)\n",
            "Requirement already satisfied: metakernel in /usr/local/lib/python3.10/dist-packages (from spylon-kernel) (0.30.2)\n",
            "Requirement already satisfied: spylon[spark] in /usr/local/lib/python3.10/dist-packages (from spylon-kernel) (0.3.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from spylon-kernel) (6.3.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.10->spylon-kernel) (0.8.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->spylon-kernel) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->spylon-kernel) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->spylon-kernel) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->spylon-kernel) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from metakernel->spylon-kernel) (5.7.2)\n",
            "Requirement already satisfied: pexpect>=4.8 in /usr/local/lib/python3.10/dist-packages (from metakernel->spylon-kernel) (4.9.0)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (from spylon[spark]->spylon-kernel) (2.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from spylon[spark]->spylon-kernel) (6.0.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->spylon-kernel) (67.7.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->spylon-kernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->spylon-kernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->spylon-kernel) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->spylon-kernel) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->spylon-kernel) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->spylon-kernel) (0.1.7)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.9.2->metakernel->spylon-kernel) (4.2.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>=4.8->metakernel->spylon-kernel) (0.7.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->spylon-kernel) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->spylon-kernel) (2.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->spylon-kernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel->spylon-kernel) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spylon_kernel install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Jm4pVAmWwC",
        "outputId": "643694a2-169a-4b89-8673-b2b23b08b0ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[InstallKernelSpec] Removing existing kernelspec in /usr/local/share/jupyter/kernels/spylon-kernel\n",
            "[InstallKernelSpec] Installed kernelspec spylon-kernel in /usr/local/share/jupyter/kernels/spylon-kernel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, upper # импортируем функции Spark SQL\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkSqlbasicexample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Создаем данные\n",
        "data = [\n",
        "    (\"John\", 30, \"2024-02-29\", True),\n",
        "    (\"Jane\", 25, \"2024-02-28\", False)\n",
        "]\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"BirthDate\", \"IsActive\"])\n",
        "\n",
        "# Выводим DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWCXTBUcVzBS",
        "outputId": "3cb13948-6b2d-4c1c-bb94-cefcb58aed77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+----------+--------+\n",
            "|Name|Age| BirthDate|IsActive|\n",
            "+----+---+----------+--------+\n",
            "|John| 30|2024-02-29|    true|\n",
            "|Jane| 25|2024-02-28|   false|\n",
            "+----+---+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример фильтрации\n",
        "filteredDf = df.filter(col(\"Age\") > 25)\n",
        "filteredDf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_83rKPGIY5h4",
        "outputId": "66e8e3c2-deaa-4ceb-84b1-066476758dce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+----------+--------+\n",
            "|Name|Age| BirthDate|IsActive|\n",
            "+----+---+----------+--------+\n",
            "|John| 30|2024-02-29|    true|\n",
            "+----+---+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример агрегации\n",
        "avgAge = df.agg(avg(\"Age\"))\n",
        "avgAge.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6sNlCg5ZEt2",
        "outputId": "4ba33aba-a066-488a-8e95-7266a893245f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|avg(Age)|\n",
            "+--------+\n",
            "|    27.5|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример преобразования\n",
        "withUppercaseName = df.withColumn(\"Name\", upper(col(\"Name\")))\n",
        "withUppercaseName.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "881ID6CvZHjL",
        "outputId": "f2eb53ba-3ce5-4c70-9cfa-6258e1296f39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+----------+--------+\n",
            "|Name|Age| BirthDate|IsActive|\n",
            "+----+---+----------+--------+\n",
            "|JOHN| 30|2024-02-29|    true|\n",
            "|JANE| 25|2024-02-28|   false|\n",
            "+----+---+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "tvK93BItZLk3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCMhQmraaqcR",
        "outputId": "cf6c7ee0-2b9f-4dc3-925c-ef7299e0e7a9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pyspark                          3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 1:\n",
        "У вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет среднее значение для каждого уникального ключа."
      ],
      "metadata": {
        "id": "VtggnLn_-HUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код выполняет следующие действия:\n",
        "\n",
        "1. Импортирует необходимые модули из библиотеки PySpark: `SparkConf` и `SparkContext`.\n",
        "2. Создает объект `SparkConf`, который представляет конфигурацию для создания `SparkContext`.\n",
        "   - Метод `setAppName(\"Average By Key Example\")` устанавливает имя приложения в \"Average By Key Example\".\n",
        "   - Метод `setMaster(\"local[*]\")` устанавливает локальный режим выполнения Spark с использованием всех доступных ядер процессора.\n",
        "3. Создает объект `SparkContext` (sc) на основе конфигурации `SparkConf`.\n",
        "4. Создает RDD (Resilient Distributed Dataset) под названием `dataRDD` с помощью метода `parallelize`, передавая список пар ключ-значение.\n",
        "5. Преобразует RDD `dataRDD`, используя цепочку операций:\n",
        "   - Метод `mapValues` применяет функцию `lambda value: (value, 1)` к каждому значению RDD, превращая его в кортеж (значение, 1).\n",
        "   - Метод `reduceByKey` выполняет суммирование значений для каждого ключа, суммируя значения и соответствующие им счетчики.\n",
        "   - Метод `mapValues` применяет функцию `lambda sum_count: sum_count[0] / sum_count[1]` к каждой сумме и счетчику, вычисляя среднее значение для каждого ключа.\n",
        "6. Вызывает операцию `collect` на RDD `averageRDD` для получения всех результатов в локальном списке.\n",
        "7. Итерируется по результатам и выводит каждую пару ключ-значение.\n",
        "8. Вызывает метод `stop` у объекта `SparkContext` для остановки Spark контекста и освобождения ресурсов.\n",
        "\n",
        "Таким образом, данный код вычисляет среднее значение для каждого ключа в RDD `dataRDD` с использованием Spark и выводит результаты./"
      ],
      "metadata": {
        "id": "JZN8fEN-Gsdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setAppName(\"Average By Key Example\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "dataRDD = sc.parallelize([\n",
        "    (\"key1\", 10),\n",
        "    (\"key2\", 20),\n",
        "    (\"key1\", 30),\n",
        "    (\"key2\", 40),\n",
        "    (\"key1\", 50)\n",
        "])\n",
        "\n",
        "averageRDD = dataRDD \\\n",
        "    .mapValues(lambda value: (value, 1)) \\\n",
        "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
        "    .mapValues(lambda sum_count: sum_count[0] / sum_count[1])\n",
        "\n",
        "# Collect the data and print it\n",
        "result = averageRDD.collect()\n",
        "for key, value in result:\n",
        "    print(key, value)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqUx9NNd8rk1",
        "outputId": "239eb71a-7697-4ce4-da06-3c9df5e1a53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key1 30.0\n",
            "key2 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 2\n",
        "\n",
        "У вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет максимальное значение для каждого уникального ключа."
      ],
      "metadata": {
        "id": "alAKvc-c91l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для выполнения операции поиска максимального значения для каждого ключа в параллельно распределенном наборе данных (RDD).\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируются необходимые модули: `SparkContext` и `SparkConf` из пакета `pyspark`.\n",
        "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"MaxValueByKeyExample\".\n",
        "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
        "4. Создается список `data`, который содержит пары ключ-значение. Каждая пара представлена в виде кортежа.\n",
        "5. Создается RDD (`rdd`) из списка `data` с помощью метода `parallelize()`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
        "6. Выполняется операция `reduceByKey(max)` на RDD `rdd`. Операция `reduceByKey()` объединяет значения для каждого ключа и применяет функцию `max` для нахождения максимального значения.\n",
        "7. Результат операции `reduceByKey()` сохраняется в переменной `max_values`.\n",
        "8. Вызывается метод `collect()` на `max_values` для сбора всех результатов в драйвере программы.\n",
        "9. Затем происходит итерация по результатам (`results`), и для каждого результата выводится ключ и соответствующее максимальное значение с помощью оператора `print()`.\n",
        "10. Наконец, вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код выполняет операцию поиска максимального значения для каждого ключа в RDD с использованием Spark."
      ],
      "metadata": {
        "id": "JeclI9ZdG6FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"MaxValueByKeyExample\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "data = [(\"key1\", 10), (\"key2\", 5), (\"key1\", 20), (\"key2\", 15), (\"key3\", 8)]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "max_values = rdd.reduceByKey(max)\n",
        "\n",
        "results = max_values.collect()\n",
        "for result in results:\n",
        "    print(result[0], result[1])\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FFxvzuW9RWx",
        "outputId": "b096de2c-c821-4d3c-a728-3243686edf6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key1 20\n",
            "key2 15\n",
            "key3 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 3\n",
        "\n",
        "Напишите программу на Apache Spark, которая считает сумму всех чисел в заданном RDD (Resilient Distributed Dataset)."
      ],
      "metadata": {
        "id": "i1Cm5nVo-z6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для вычисления суммы элементов в распределенном наборе данных (RDD).\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируются необходимые модули: `SparkContext` и `SparkConf` из пакета `pyspark`.\n",
        "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"SumRDDExample\".\n",
        "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
        "4. Создается список `data`, который содержит числа.\n",
        "5. Создается RDD (`rdd`) из списка `data` с помощью метода `parallelize()`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
        "6. Выполняется операция `sum()` на RDD `rdd`. Операция `sum()` вычисляет сумму всех элементов в RDD.\n",
        "7. Результат операции `sum()` сохраняется в переменной `total_sum`.\n",
        "8. Выводится сообщение с помощью оператора `print()`, которое содержит сумму всех чисел.\n",
        "9. Вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код вычисляет сумму всех чисел в RDD с использованием Spark и выводит эту сумму на экран."
      ],
      "metadata": {
        "id": "UPL9cHrJHG7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"SumRDDExample\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "total_sum = rdd.sum()\n",
        "\n",
        "print(\"Сумма всех чисел:\", total_sum)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ-i7Ezb-TW6",
        "outputId": "5bc843c0-f21d-407d-a52e-63730a041bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сумма всех чисел: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 4\n",
        "\n",
        "У вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет топ N ключей с наибольшим значением, где N - заданное число."
      ],
      "metadata": {
        "id": "Dc--9guc-rL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для нахождения N ключей с наибольшими значениями в параллельно распределенном наборе данных (RDD).\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируются необходимые модули: `SparkContext` и `SparkConf` из пакета `pyspark`.\n",
        "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"TopNKeysExample\".\n",
        "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
        "4. Создается список `data`, который содержит пары ключ-значение. Каждая пара представлена в виде кортежа.\n",
        "5. Создается RDD (`rdd`) из списка `data` с помощью метода `parallelize()`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
        "6. Устанавливается значение переменной `N`, которая определяет количество ключей с наибольшими значениями, которые будут выбраны.\n",
        "7. Выполняется операция `takeOrdered(N, key=lambda x: -x[1])` на RDD `rdd`. Операция `takeOrdered()` возвращает N элементов RDD, отсортированных в порядке возрастания на основе заданного ключа. В данном случае, ключом является второй элемент кортежа (значение), и сортировка осуществляется в порядке убывания, указав `-x[1]`.\n",
        "8. Результат операции `takeOrdered()` сохраняется в переменной `top_N_keys`.\n",
        "9. Затем происходит итерация по результатам (`top_N_keys`), и для каждой пары ключ-значение выводится ключ и значение с помощью оператора `print()`.\n",
        "10. Наконец, вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код находит N ключей с наибольшими значениями в RDD и выводит их на экран в порядке убывания значений."
      ],
      "metadata": {
        "id": "pt_Jo1HWHUWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"TopNKeysExample\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "data = [(\"key1\", 10), (\"key2\", 5), (\"key3\", 20), (\"key4\", 15), (\"key5\", 8)]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "N = 3\n",
        "\n",
        "top_N_keys = rdd.takeOrdered(N, key=lambda x: -x[1])\n",
        "\n",
        "for key, value in top_N_keys:\n",
        "    print(key, value)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwsctcXm-h91",
        "outputId": "daf249ed-3d17-44a4-92bf-5061eb6e25bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key3 20\n",
            "key4 15\n",
            "key1 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 5\n",
        "\n",
        "Напишите программу на Apache Spark, которая читает CSV-файл и выводит количество строк в файле."
      ],
      "metadata": {
        "id": "pgA14vPX_FSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для подсчета количества строк в CSV-файле с использованием Spark SQL и DataFrame API.\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируются необходимые модули: `SparkContext`, `SparkConf` из пакета `pyspark`, а также `SparkSession` из модуля `pyspark.sql`.\n",
        "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"CountCSVLinesExample\".\n",
        "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
        "4. Создается объект `SparkSession` (`spark`) с помощью метода `SparkSession.builder.getOrCreate()`. `SparkSession` предоставляет доступ к функциональности Spark SQL.\n",
        "5. Устанавливается путь к CSV-файлу, который будет использоваться для подсчета количества строк. Путь указывается в переменной `csv_file_path`.\n",
        "6. С помощью метода `read.csv()` на объекте `spark` загружается CSV-файл. Методу передаются следующие аргументы: `csv_file_path` - путь к файлу, `header=True` - указывает, что первая строка файла содержит заголовки столбцов, `inferSchema=True` - указывает на автоматическое определение схемы данных.\n",
        "7. Результат чтения CSV-файла сохраняется в переменной `df` в виде DataFrame. DataFrame - это распределенная коллекция данных, организованная в именованные столбцы.\n",
        "8. Выполняется операция `count()` на объекте `df`. Операция `count()` возвращает количество строк в DataFrame.\n",
        "9. Результат операции `count()` сохраняется в переменной `line_count`.\n",
        "10. Выводится сообщение с помощью оператора `print()`, которое содержит количество строк в файле.\n",
        "11. Вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код загружает CSV-файл с использованием Spark SQL, подсчитывает количество строк в файле и выводит это число на экран."
      ],
      "metadata": {
        "id": "-6Qcgg4QHkAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName(\"CountCSVLinesExample\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "csv_file_path = \"path/to/your/csv/file.csv\"\n",
        "\n",
        "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "\n",
        "line_count = df.count()\n",
        "\n",
        "print(\"Количество строк в файле:\", line_count)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "sSdbw1op-ryL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 6\n",
        "\n",
        "У вас есть RDD со следующей структурой: (ключ, значение). Напишите программу на Apache Spark, которая найдет сумму значений для каждого уникального ключа, но только для ключей, значение которых больше заданного порога."
      ],
      "metadata": {
        "id": "5VwGK-B6_KrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для фильтрации и суммирования значений, превышающих заданный порог, в распределенном наборе данных (RDD).\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируются необходимые модули: `SparkContext` и `SparkConf` из пакета `pyspark`.\n",
        "2. Создается объект `SparkConf` с настройками для приложения Spark. В данном случае, устанавливается имя приложения \"SumValuesAboveThreshold\".\n",
        "3. Создается объект `SparkContext` (`sc`), который представляет соединение с кластером Spark с использованием настроек из `SparkConf`.\n",
        "4. Создается список `data`, который содержит пары ключ-значение. Каждая пара представлена в виде кортежа.\n",
        "5. Создается RDD (`rdd`) из списка `data` с помощью метода `parallelize()`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
        "6. Устанавливается значение переменной `threshold`, которое определяет пороговое значение для фильтрации.\n",
        "7. Выполняется операция `filter(lambda x: x[1] > threshold)` на RDD `rdd`. Операция `filter()` фильтрует элементы RDD с использованием заданного условия, в данном случае, значения второго элемента кортежа должны быть больше порогового значения.\n",
        "8. Результат фильтрации сохраняется в переменной `filtered_rdd`.\n",
        "9. Выполняется операция `reduceByKey(lambda a, b: a + b)` на `filtered_rdd`. Операция `reduceByKey()` суммирует значения для каждого ключа в RDD. В данном случае, значения суммируются с использованием лямбда-функции `lambda a, b: a + b`.\n",
        "10. Результат операции `reduceByKey()` сохраняется в переменной `result`.\n",
        "11. Затем происходит итерация по результатам (`result`), и для каждой пары ключ-значение выводится ключ и значение с помощью оператора `print()`.\n",
        "12. Наконец, вызывается метод `stop()` на объекте `SparkContext` (`sc`), чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код фильтрует значения в RDD, оставляя только те, которые превышают заданный порог, а затем суммирует значения для каждого ключа и выводит результаты на экран."
      ],
      "metadata": {
        "id": "rPCmBOYuHzze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"SumValuesAboveThreshold\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "data = [(\"key1\", 10), (\"key2\", 20), (\"key1\", 30), (\"key3\", 40), (\"key2\", 50)]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "threshold = 25\n",
        "\n",
        "filtered_rdd = rdd.filter(lambda x: x[1] > threshold)\n",
        "\n",
        "result = filtered_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "for key, value in result.collect():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpSR4tWS_ZKH",
        "outputId": "f870f1dd-8d50-4b41-a30a-321abf72199a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key1: 30\n",
            "key2: 50\n",
            "key3: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 7\n",
        "\n",
        "У вас есть RDD со следующей структурой: (ключ, список значений). Напишите программу на Apache Spark, которая найдет среднюю длину списка значений для каждого ключа."
      ],
      "metadata": {
        "id": "VgbSwOTcABqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для нахождения средней длины списков значений для каждого ключа в распределенном наборе данных (RDD) с использованием Spark SQL и DataFrame API.\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируется модуль `SparkSession` из пакета `pyspark.sql`.\n",
        "2. Создается объект `spark` с помощью метода `SparkSession.builder.appName(\"AverageListLength\").getOrCreate()`. `SparkSession` предоставляет доступ к функциональности Spark SQL.\n",
        "3. Создается список `data`, который содержит пары ключ-список значений. Каждая пара представлена в виде кортежа.\n",
        "4. Создается RDD (`rdd`) из списка `data` с помощью метода `spark.sparkContext.parallelize(data)`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
        "5. Выполняется операция `mapValues(lambda x: len(x))` на RDD `rdd`. Операция `mapValues()` применяет заданную функцию к каждому значению в RDD, в данном случае, функция `lambda x: len(x)` возвращает длину списка значений.\n",
        "6. Результат операции `mapValues()` сохраняется в переменной `rdd_key_length`.\n",
        "7. Выполняется операция `combineByKey()` на `rdd_key_length`. Операция `combineByKey()` используется для агрегации значений по ключу. В данном случае, используются три функции:\n",
        "   - Первая функция `lambda x: (x, 1)` преобразует каждое значение в кортеж `(значение, 1)`.\n",
        "   - Вторая функция `lambda acc, x: (acc[0] + x, acc[1] + 1)` объединяет значения для каждого ключа, складывая длины и подсчитывая количество списков значений.\n",
        "   - Третья функция `lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])` объединяет результаты агрегации для разных партиций или ключей.\n",
        "8. Результат операции `combineByKey()` сохраняется в переменной `rdd_sum_count`.\n",
        "9. Выполняется операция `mapValues(lambda x: x[0] / x[1])` на `rdd_sum_count`. Операция `mapValues()` применяет заданную функцию к каждому значению в RDD, в данном случае, функция `lambda x: x[0] / x[1]` вычисляет среднюю длину путем деления суммы длин на количество списков значений.\n",
        "10. Результат операции `mapValues()` сохраняется в переменной `rdd_avg_length`.\n",
        "11. Выполняется операция `collect()` на `rdd_avg_length`, чтобы получить все результаты.\n",
        "12. Затем происходит итерация по результатам (`result`), и для каждого ключа и средней длины выводится сообщение с помощью оператора `print()`.\n",
        "13. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код находит среднюю длину списков значений для каждого ключа в RDD и выводит результаты на экран."
      ],
      "metadata": {
        "id": "R0PzIHdkID1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AverageListLength\").getOrCreate()\n",
        "\n",
        "data = [(\"key1\", [1, 2, 3]),\n",
        "        (\"key2\", [4, 5]),\n",
        "        (\"key1\", [6, 7, 8, 9]),\n",
        "        (\"key2\", [10, 11, 12])]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "rdd_key_length = rdd.mapValues(lambda x: len(x))\n",
        "\n",
        "rdd_sum_count = rdd_key_length.combineByKey(\n",
        "    lambda x: (x, 1),\n",
        "    lambda acc, x: (acc[0] + x, acc[1] + 1),\n",
        "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        ")\n",
        "\n",
        "rdd_avg_length = rdd_sum_count.mapValues(lambda x: x[0] / x[1])\n",
        "\n",
        "result = rdd_avg_length.collect()\n",
        "for key, avg_length in result:\n",
        "    print(\"Key: {}, Average Length: {}\".format(key, avg_length))\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1orWdv3c_Z4C",
        "outputId": "45c528e2-589b-4dfd-afae-c5cd6ec1527e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key: key1, Average Length: 3.5\n",
            "Key: key2, Average Length: 2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 8\n",
        "\n",
        "У вас есть RDD, содержащий записи о посещениях веб-сайта со следующей структурой: (пользователь, дата, время). Напишите программу на Apache Spark, которая найдет количество уникальных пользователей для каждой даты."
      ],
      "metadata": {
        "id": "YoQRHVhPAMmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для подсчета уникальных пользователей для каждой даты в распределенном наборе данных (RDD) с использованием Spark SQL и DataFrame API.\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируется модуль `SparkSession` из пакета `pyspark.sql`.\n",
        "2. Создается объект `spark` с помощью метода `SparkSession.builder.appName(\"UniqueUsersPerDate\").getOrCreate()`. `SparkSession` предоставляет доступ к функциональности Spark SQL.\n",
        "3. Создается список `data`, который содержит записи о пользователях с указанием имени пользователя, даты и времени.\n",
        "4. Создается RDD (`rdd`) из списка `data` с помощью метода `spark.sparkContext.parallelize(data)`. RDD - это распределенный неизменяемый набор данных, на котором можно выполнять параллельные операции.\n",
        "5. Выполняется операция `map(lambda x: (x[1], x[0]))` на RDD `rdd`. Операция `map()` применяет заданную функцию к каждому элементу в RDD, в данном случае, функция `lambda x: (x[1], x[0])` меняет местами значение даты и имени пользователя, чтобы ключом стала дата.\n",
        "6. Результат операции `map()` сохраняется в переменной `rdd_date_user`.\n",
        "7. Выполняется операция `distinct()` на `rdd_date_user`. Операция `distinct()` удаляет дублирующиеся элементы из RDD и оставляет только уникальные значения.\n",
        "8. Результат операции `distinct()` сохраняется в переменной `rdd_unique_users`.\n",
        "9. Выполняется операция `countByKey()` на `rdd_unique_users`. Операция `countByKey()` подсчитывает количество значений для каждого ключа в RDD и возвращает словарь, где ключами являются уникальные даты, а значениями - количество уникальных пользователей для каждой даты.\n",
        "10. Результат операции `countByKey()` сохраняется в переменной `result`.\n",
        "11. Затем происходит итерация по результатам (`result`), и для каждой даты и количества уникальных пользователей выводится сообщение с помощью оператора `print()`.\n",
        "12. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код находит уникальных пользователей для каждой даты в RDD и выводит результаты на экран."
      ],
      "metadata": {
        "id": "mABqh9qZIT-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"UniqueUsersPerDate\").getOrCreate()\n",
        "\n",
        "data = [(\"user1\", \"2024-04-01\", \"10:00:00\"),\n",
        "        (\"user2\", \"2024-04-01\", \"11:30:00\"),\n",
        "        (\"user3\", \"2024-04-02\", \"09:45:00\"),\n",
        "        (\"user1\", \"2024-04-02\", \"14:20:00\"),\n",
        "        (\"user2\", \"2024-04-03\", \"16:30:00\"),\n",
        "        (\"user3\", \"2024-04-03\", \"18:15:00\")]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "rdd_date_user = rdd.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "rdd_unique_users = rdd_date_user.distinct()\n",
        "\n",
        "result = rdd_unique_users.countByKey()\n",
        "\n",
        "for date, count in result.items():\n",
        "    print(\"Date: {}, Unique Users: {}\".format(date, count))\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Psq0oZAAsd",
        "outputId": "b197b28e-a91f-430a-b322-b6bdb057596e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2024-04-02, Unique Users: 2\n",
            "Date: 2024-04-03, Unique Users: 2\n",
            "Date: 2024-04-01, Unique Users: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 9\n",
        "\n",
        "У вас есть два RDD с одинаковой структурой: (ключ, значение). Напишите программу на Apache Spark, которая выполнит объединение (join) этих RDD по ключу и найдет сумму значений для каждого уникального ключа."
      ],
      "metadata": {
        "id": "kYWcfsR5Ahmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для объединения двух распределенных наборов данных (RDD) по ключу и вычисления суммы значений для каждого ключа с использованием Spark SQL и DataFrame API.\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируется модуль `SparkSession` из пакета `pyspark.sql`.\n",
        "2. Создается объект `spark` с помощью метода `SparkSession.builder.appName(\"RDDJoinAndSum\").getOrCreate()`. `SparkSession` предоставляет доступ к функциональности Spark SQL.\n",
        "3. Создается список `data1`, который содержит пары ключ-значение.\n",
        "4. Создается RDD (`rdd1`) из списка `data1` с помощью метода `spark.sparkContext.parallelize(data1)`.\n",
        "5. Создается список `data2`, который содержит пары ключ-значение.\n",
        "6. Создается RDD (`rdd2`) из списка `data2` с помощью метода `spark.sparkContext.parallelize(data2)`.\n",
        "7. Выполняется операция `join(rdd2)` на RDD `rdd1`. Операция `join()` объединяет два RDD по ключу, в данном случае, по ключу \"key\". Результат операции сохраняется в переменной `rdd_join`.\n",
        "8. Выполняется операция `mapValues(lambda x: x[0] + x[1])` на `rdd_join`. Операция `mapValues()` применяет заданную функцию к каждому значению в RDD, в данном случае, функция `lambda x: x[0] + x[1]` складывает значения по ключу. Результат операции сохраняется в переменной `rdd_sum`.\n",
        "9. Выполняется операция `collect()` на `rdd_sum`, чтобы получить все результаты.\n",
        "10. Затем происходит итерация по результатам (`result`), и для каждого ключа и суммы значений выводится сообщение с помощью оператора `print()`.\n",
        "11. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код объединяет два RDD по ключу и вычисляет сумму значений для каждого ключа, выводя результаты на экран."
      ],
      "metadata": {
        "id": "XRFs6Y4qIjfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RDDJoinAndSum\").getOrCreate()\n",
        "\n",
        "data1 = [(\"key1\", 1),\n",
        "         (\"key2\", 2),\n",
        "         (\"key3\", 3)]\n",
        "\n",
        "rdd1 = spark.sparkContext.parallelize(data1)\n",
        "\n",
        "data2 = [(\"key1\", 4),\n",
        "         (\"key2\", 5),\n",
        "         (\"key4\", 6)]\n",
        "\n",
        "rdd2 = spark.sparkContext.parallelize(data2)\n",
        "\n",
        "rdd_join = rdd1.join(rdd2)\n",
        "\n",
        "rdd_sum = rdd_join.mapValues(lambda x: x[0] + x[1])\n",
        "\n",
        "result = rdd_sum.collect()\n",
        "for key, sum_value in result:\n",
        "    print(\"Key: {}, Sum: {}\".format(key, sum_value))\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHsMywoMAap-",
        "outputId": "393e9c7a-28bc-4938-ebae-8677d0097df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key: key1, Sum: 5\n",
            "Key: key2, Sum: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задача 10\n",
        "\n",
        "Подсчет количества уникальных слов в тексте с использованием DataFrame"
      ],
      "metadata": {
        "id": "erFriTM-E-56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для подсчета количества уникальных слов в заданном тексте с использованием Spark SQL и DataFrame API.\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируются модули `SparkSession`, `explode`, `split` и `StringType` из соответствующих пакетов.\n",
        "2. Создается объект `spark` с помощью метода `SparkSession.builder`. Устанавливается имя приложения с помощью `appName(\"CountUniqueWords\")`.\n",
        "3. Указывается режим запуска приложения с помощью `master(\"local[*]\")`, где `\"local[*]\"` означает запуск приложения локально с использованием всех доступных ядер процессора.\n",
        "4. Вызывается метод `getOrCreate()` на объекте `spark` для создания или получения существующего сеанса Spark.\n",
        "5. Задается текст в виде списка строк `text`, который будет использован для создания DataFrame.\n",
        "6. Создается DataFrame `textDF` с помощью метода `spark.createDataFrame(text, StringType()).toDF(\"text\")`. Здесь каждая строка текста представляется как отдельная запись в DataFrame.\n",
        "7. Выполняется операция `split(\"text\", \" \")` на столбце `\"text\"` DataFrame `textDF`. Операция `split()` разделяет каждую строку текста на отдельные слова, используя пробел как разделитель.\n",
        "8. Выполняется операция `explode(...).alias(\"word\")` на результате операции `split()`. Операция `explode()` создает новую строку для каждого элемента в массиве слов. Результат операции сохраняется в DataFrame `wordsDF` с столбцом `\"word\"`.\n",
        "9. Выполняется операция `distinct()` на DataFrame `wordsDF`. Операция `distinct()` удаляет дублирующиеся строки и оставляет только уникальные слова.\n",
        "10. Результат операции `distinct()` сохраняется в DataFrame `uniqueWordsDF`.\n",
        "11. Выполняется операция `count()` на DataFrame `uniqueWordsDF` для подсчета количества уникальных слов.\n",
        "12. Результат подсчета сохраняется в переменной `uniqueWordCount`.\n",
        "13. Выводится сообщение с помощью оператора `print()` с указанием количества уникальных слов.\n",
        "14. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код создает DataFrame из заданного текста, разбивает текст на отдельные слова, находит уникальные слова и подсчитывает их количество, выводя результат на экран."
      ],
      "metadata": {
        "id": "AW98kvt7Ix63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CountUniqueWords\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "text = [\"Hello Spark\", \"Hello Scala\", \"Spark is awesome\"]\n",
        "textDF = spark.createDataFrame(text, StringType()).toDF(\"text\")\n",
        "wordsDF = textDF.select(explode(split(\"text\", \" \")).alias(\"word\"))\n",
        "uniqueWordsDF = wordsDF.distinct()\n",
        "uniqueWordCount = uniqueWordsDF.count()\n",
        "\n",
        "print(\"Unique word count:\", uniqueWordCount)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__veW2EaEDmH",
        "outputId": "d6a63f9e-f994-438b-d0e9-191b44499804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique word count: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код использует PySpark для создания объекта DataFrame из заданного списка словарей, фильтрации DataFrame по возрасту и вывода результата на экран.\n",
        "\n",
        "Вот пошаговое описание кода:\n",
        "\n",
        "1. Импортируется модуль `SparkSession` из пакета `pyspark.sql`.\n",
        "2. Создается объект `spark` с помощью метода `SparkSession.builder`. Устанавливается имя приложения с помощью `appName(\"OOP Example\")`.\n",
        "3. Вызывается метод `getOrCreate()` на объекте `spark` для создания или получения существующего сеанса Spark.\n",
        "4. Задается список словарей `data`, представляющий данные, из которых будет создан DataFrame.\n",
        "5. Создается DataFrame `df` с помощью метода `spark.createDataFrame(data)`. DataFrame создается на основе заданного списка словарей, где каждый словарь представляет отдельную запись в DataFrame.\n",
        "6. Выполняется операция фильтрации на DataFrame `df` с помощью метода `filter(df.age > 30)`. Здесь фильтр применяется к столбцу `age` и оставляет только записи, где значение столбца `age` больше 30.\n",
        "7. Результат фильтрации сохраняется в DataFrame `df_filtered`.\n",
        "8. Вызывается метод `show()` на DataFrame `df_filtered`, чтобы вывести результат фильтрации на экран.\n",
        "9. Вызывается метод `stop()` на объекте `spark`, чтобы закрыть соединение с кластером Spark.\n",
        "\n",
        "Таким образом, данный код создает DataFrame из заданного списка словарей, фильтрует записи по возрасту (>30) и выводит результат фильтрации на экран."
      ],
      "metadata": {
        "id": "o0a2WO_0KSqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание объекта SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"OOP Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data = [\n",
        "    {\"name\": \"Alice\", \"age\": 25},\n",
        "    {\"name\": \"Bob\", \"age\": 30},\n",
        "    {\"name\": \"Charlie\", \"age\": 35}\n",
        "]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df_filtered = df.filter(df.age > 30)\n",
        "df_filtered.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "L5o_byrTKTGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}